#!/usr/bin/env python3
"""
Run a pilot experiment to verify the teaching environment.

Usage:
    python scripts/run_pilot.py
    python scripts/run_pilot.py --questions 5
    python scripts/run_pilot.py --categories math physics chemistry
"""

import argparse
import sys
import time
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def print_header(text: str):
    """Print a formatted header."""
    print()
    print("=" * 70)
    print(f"  {text}")
    print("=" * 70)


def print_section(text: str):
    """Print a section header."""
    print()
    print(f"── {text} " + "─" * (66 - len(text)))


def main():
    parser = argparse.ArgumentParser(
        description="Run a pilot teaching experiment",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/run_pilot.py                    # Default: 2 questions (math, physics)
  python scripts/run_pilot.py -n 5               # 5 questions per category
  python scripts/run_pilot.py -c math biology    # Specific categories
  python scripts/run_pilot.py -t 10              # 10 max turns per question
        """
    )
    parser.add_argument("-n", "--questions", type=int, default=1,
                        help="Questions per category (default: 1)")
    parser.add_argument("-c", "--categories", nargs="+", default=["math", "physics"],
                        help="Categories to include (default: math physics)")
    parser.add_argument("-t", "--max-turns", type=int, default=5,
                        help="Max teacher turns per question (default: 5)")
    parser.add_argument("--name", type=str, default="pilot",
                        help="Experiment name (default: pilot)")
    args = parser.parse_args()
    
    # Imports (after path setup)
    from src.environment import (
        ExperimentConfig,
        ExperimentRunner,
        EnvironmentConfig,
        SamplingConfig,
        TEACHER_CONFIGS,
        DEFAULT_STUDENT_CONFIG,
        load_mmlu_pro_from_huggingface,
        sample_questions,
    )
    from src.environment.llm_client import create_llm_client
    
    print_header("TEACHING ENVIRONMENT PILOT")
    
    # ─────────────────────────────────────────────────────────────────────────
    # Configuration
    # ─────────────────────────────────────────────────────────────────────────
    print_section("Configuration")
    
    sampling = SamplingConfig(
        questions_per_category=args.questions,
        categories=tuple(args.categories),
    )
    
    config = ExperimentConfig(
        name=args.name,
        description=f"Pilot experiment: {args.questions} questions/category from {args.categories}",
        teacher_config=TEACHER_CONFIGS[0],
        student_config=DEFAULT_STUDENT_CONFIG,
        env_config=EnvironmentConfig(
            max_teacher_turns=args.max_turns,
            sampling=sampling,
        ),
        output_dir=Path("experiments"),
    )
    
    print(f"  Experiment:  {config.name}")
    print(f"  Teacher:     {config.teacher_config.model_id}")
    print(f"  Student:     {config.student_config.model_id}")
    print(f"  Categories:  {', '.join(args.categories)}")
    print(f"  Questions:   {args.questions} per category")
    print(f"  Max turns:   {args.max_turns}")
    
    # ─────────────────────────────────────────────────────────────────────────
    # Load Dataset
    # ─────────────────────────────────────────────────────────────────────────
    print_section("Loading Dataset")
    
    print("  Fetching MMLU-Pro from HuggingFace...", end=" ", flush=True)
    all_questions = load_mmlu_pro_from_huggingface(stratified=False)
    print(f"✓ {len(all_questions):,} questions")
    
    sampled = sample_questions(all_questions, sampling)
    print(f"  Sampled: {len(sampled)} questions")
    
    # ─────────────────────────────────────────────────────────────────────────
    # Verify API Connection
    # ─────────────────────────────────────────────────────────────────────────
    print_section("Verifying API Connection")
    
    try:
        client = create_llm_client()
        print("  ✓ Client created")
    except ValueError as e:
        print(f"  ✗ Error: {e}")
        print()
        print("  Make sure TOGETHER_API_KEY is set in your .env file")
        return 1
    
    # Test API with timing
    print("  Testing API call...", end=" ", flush=True)
    t0 = time.time()
    try:
        response = client.chat_with_metadata(
            model=config.teacher_config.model_id,
            messages=[{"role": "user", "content": "Reply with exactly: API_OK"}],
            max_tokens=10,
            temperature=0.0,
        )
        t1 = time.time()
        
        if "API_OK" in response.content or "API" in response.content:
            print(f"✓ Response in {t1-t0:.2f}s")
            print(f"  ✓ Model: {response.model}")
            print(f"  ✓ Tokens: {response.prompt_tokens} in, {response.completion_tokens} out")
        else:
            print(f"⚠ Unexpected response: {response.content[:50]}")
    except Exception as e:
        print(f"✗ Failed: {e}")
        return 1
    
    # ─────────────────────────────────────────────────────────────────────────
    # Run Experiment
    # ─────────────────────────────────────────────────────────────────────────
    print_section("Running Experiment")
    print()
    
    runner = ExperimentRunner(config, client)
    output = runner.run(sampled, show_progress=True)
    
    # ─────────────────────────────────────────────────────────────────────────
    # Save Results
    # ─────────────────────────────────────────────────────────────────────────
    print_section("Saving Results")
    
    experiment_id = config.experiment_id
    saved = output.save(config.output_dir, experiment_id)
    
    print(f"  Folder:  {saved['folder']}")
    print(f"  Config:  config.yaml")
    print(f"  Summary: summary.json")
    print(f"  Results: results.json")
    
    # ─────────────────────────────────────────────────────────────────────────
    # Summary
    # ─────────────────────────────────────────────────────────────────────────
    print_section("Experiment Summary")
    
    stats = output.stats
    print(f"  Sessions completed:    {stats['num_sessions']}")
    print(f"  Initial correct:       {stats['num_initial_correct']}")
    print(f"  Terminated naturally:  {stats['num_terminated_naturally']}")
    print(f"  Forced termination:    {stats['num_forced_termination']}")
    print(f"  Avg turns/question:    {stats['avg_teacher_turns']:.1f}")
    print(f"  Total API calls:       {stats.get('total_api_calls', 'N/A')}")
    print(f"  Total time:            {output.total_duration_seconds:.1f}s")
    print(f"  Avg time/question:     {stats.get('avg_time_per_question', 0):.1f}s")
    
    print()
    print(f"  Actions used:")
    for action, count in sorted(stats.get('action_usage', {}).items(), key=lambda x: -x[1]):
        print(f"    {action}: {count}")
    
    print_header("PILOT COMPLETE")
    print(f"  Results saved to: {saved['folder']}")
    print()
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
